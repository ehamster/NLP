词的分布式表示有三种
================

```bash
1.基于矩阵的分布表示 
  矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。由于分布假说认为上下文相似的词，其语义也相似，
  因此在这种表示下，两个词的语义相似度可以直接转化为两个向量的空间距离。

2. 基于聚类的分布表示

3.基于神经网络的分布表示，词嵌入（ word embedding）
  在前面基于矩阵的分布表示方法中，最常用的上下文是词。如果使用包含词序信息的 n-gram 作为上下文，当 n 增加时， n-gram 的总数会呈指数级增长，
  此时会遇到维数灾难问题。而神经网络在表示 n-gram 时，可以通过一些组合方式对 n 个词进行组合，参数个数仅以线性速度增长。
  有了这一优势，神经网络模型可以对更复杂的上下文进行建模，在词向量中包含更丰富的语义信息。
  

```
神经网络语言模型 
===========

```bash
a) Neural Network Language Model ，NNLM
b) Log-Bilinear Language Model， LBL
c) Recurrent Neural Network based Language Model，RNNLM
d) Collobert 和 Weston 在2008 年提出的 C&W 模型
e) Mikolov 等人提出了 CBOW（ Continuous Bagof-Words）和 Skip-gram 模型


1.词嵌入是训练语言模型的副产品。最经典的两个模型 CBOW and skip-Gram
  CBOW也是统计语言模型的一种，顾名思义就是根据某个词前面的C个词或者前后C个连续的词，来计算某个词出现的概率。
  Skip-Gram Model相反，是根据某个词，然后分别计算它前后出现某几个词的各个概率。

2.CBOW
  以“我爱北京天安门”这句话为例。假设我们现在关注的词是“爱”，C＝2时它的上下文分别是“我”，“北京天安门”。CBOW模型就是把“我” “北京天安门” 的one hot
  表示方式作为输入，也就是C个1xV的向量，分别跟同一个VxN的大小的系数矩阵W1相乘得到C个1xN的隐藏层hidden layer，然后C个取平均所以只算一个隐藏层。
  这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。然后再跟另一个NxV大小的系数矩阵W2相乘得到1xV的输出层，
  这个输出层每个元素代表的就是词库里每个词的事后概率。输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。
  这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，
  word2vec就用基于huffman编码的Hierarchical softmax筛选掉了一部分不可能的词，然后又用nagetive samping再去掉了一些负样本的词所以时间复杂度
  就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。

3.Word embedding的训练方法大致可以分为两类
  a)一类是无监督或弱监督的预训练
    无监督或弱监督的预训练以word2vec和auto-encoder为代表。这一类模型的特点是，不需要大量的人工标记样本就可以得到质量还不错的embedding向量。
    不过因为缺少了任务导向，可能和我们要解决的问题还有一定的距离。因此，我们往往会在得到预训练的embedding向量后，
    用少量人工标注的样本去fine-tune整个模型
    
  b)端对端的有监督模型
    端对端模型学习到的embedding向量也往往更加准确。例如，通过一个embedding层和若干个卷积层连接而成的深度神经网络以实现对句子的情感分类，
    可以学习到语义更丰富的词向量表达。
    
  c)
  另外一点很实用的建议，在你做某一项具体的NLP任务时如你要用到词向量，那么我建议你：要么1、选择使用别人训练好的词向量，
  注意，得使用相同语料内容领域的词向量；要么2、自己训练自己的词向量。我建议是前者，因为……坑太多了。


```
