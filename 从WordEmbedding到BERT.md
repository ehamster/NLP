1.预训练-用别人的底层参数
====================

 a)由来
 -----------------
  ```bash
   做图像分类问题，训练数据太少怎么办？先用其他多的数据做一个任务A，然后保存底层参数
   用一样的网络结构和我们的数据做任务B，用了任务A得到的网络参数的浅层。
     Frozen:在训练我们的任务B的时候浅层参数不变
     Fine-tuning： 就是更好地把参数进行调整使得更适应当前的 C 任务。一般图像或者视频领域要做预训练一般都这么做
   为什么可以用？
    因为观察所有模型底层，发现底层学习到的特征打印发现都是最基础的线条信息，是可以广泛使用的
    一般用imageNet做预训练，数据多，种类多
  ```
  
  b)NLP预训练
  ----------------
  ```bash
    Word Embedding
    就是当年做语言模型(根据前文判断一句话是人话的概率)的副产品,相当于模型第一层的参数
    Word2Vec有两种方法，CBOW或者Skipgram ，word2vec是根据上下文了，为什么这么随性？因为这个模型的目的不是为了做什么任务，
    而是为了得到word embedding这一层的参数。
    所以和图像类似，不过word embedding是最底层的一层参数
    如何使用？
      句子每个单词onehot输入，乘以学好的word embedding矩阵Q
    这是2018年之前的方法，WE有个问题是无法区分多义词   play有玩和播放的意思，但是WE里都是一个向量表示,所以提升效果不大
    
  ```
  
  2.从WE进化到ELMO
  =====================
  
  a)ELMO思想：
  ----------------
  ```bash
    我事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，
    单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，
    这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了
    
  ```
  b)ELMO结构
  -----------------------
  ```bash
  两部分：语言模型帷幕的的预训练和下游任务部分
  1.预训练是双层双向LSTM：
  根据上下文预测wi
  先用预训练部分训练大量语料，然后输入一个新句子，每个单词会得到3个embedding：1.底层word embedding 2.LSTM中对应单词位置的 Embedding，句法信息更多一些
  3.第二层 LSTM 中对应单词位置的 Embedding，这层编码单词的语义信息更多一些
  然后每个单词3个embedding加权求和作为下游任务输入，这个预训练也叫Feature-based Pre-Training
  2.下游任务
  
  解决的多意问题
  弱点：BILSTM抽取特征能力没有transform厉害
 
  ```
  
  3.GPT generative pre training
  ======================
  
  1.结构
  ----------
  ```bash
  也是两层，语言模型为目的的预训练和通过fine-tuning模式解决下游任务，类似于ELMO不同在于
  a)特征提取使用了transform，强于LSTM transform易于并行，长距离特征强
  b)预训练没使用上下文而是只使用了上文(目前看来不太好，还是上下文比较好)
  
  第二层必须和GPT网络结构一样，论文有施工图
  ```
  
  4.BERT
  ==========================
  ```bash
  也是两层
  1.和GPT的区别就是预训练采用双向的语言模型，类似于ELMO
 2.第二层就是Fine-turning模式解决下游任务，他的预训练语言模型使用的数据大
 
 所以下游任务也要改造结构
 ```
 
 NLP四大类任务
 --------------
 ```bash
 a)序列标注： 分词/POS tag /NER/语义标注
 b)分类任务： 文本分类/情感分析
 c)句子关系判断： entailment/QA/自然语言推理
 d)生成式任务：机器翻译/文本摘要
 
 ```
 BERT如何构造双向语言模型
 ----------------
 ```bash
 1.Masked双向语言模型
 Masked 双向语言模型向上图展示这么做：随机选择语料中 15% 的单词，把它抠掉，也就是用 [Mask] 掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。
 但是这里有个问题：训练过程大量看到 [mask] 标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对 [mask] 这个标记的，
 但是实际使用又见不到这个标记，这自然会有问题。

为了避免这个问题，Bert 改造了一下，15% 的被上天选中要执行 [mask] 替身这项光荣任务的单词中，只有 80% 真正被替换成 [mask] 标记，
10% 被狸猫换太子随机替换成另外一个单词，10% 情况这个单词还待在原地不做改动。这就是 Masked 双向语音模型的具体做法。

2.Next Sentence Prediction
指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，
随机选择一个拼到第一个句子后面。我们要求模型除了做上述的 Masked 语言模型任务外，附带再做个句子关系预测，
判断第二个句子是不是真的是第一个句子的后续句子。

之所以这么做，是考虑到很多 NLP 任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。
所以可以看到，它的预训练是个多任务过程。这也是 Bert 的一个创新。

其实NSP效果不大，只是对特定任务有影响

 ```
 改造下游任务
 ------------------
 ```bash
 1.对于句子关系类任务，很简单，和 GPT 类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，
 把第一个起始符号对应的 Transformer 最后一层位置上面串接一个 softmax 分类层即可。
 2.对于分类问题，与 GPT 一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造
 3.对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分 Transformer 最后一层每个单词对应位置都进行分类即可
 4.生成没提，。只需要附着在 S2S 结构上，encoder 部分是个深度 Transformer 结构，decoder 部分也是个深度 Transformer 结构。
 根据任务选择不同的预训练数据初始化 encoder 和 decoder 即可。
 4.
 ```
 
