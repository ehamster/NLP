1.预训练-用别人的底层参数
====================

 a)由来
 -----------------
  ```bash
   做图像分类问题，训练数据太少怎么办？先用其他多的数据做一个任务A，然后保存底层参数
   用一样的网络结构和我们的数据做任务B，用了任务A得到的网络参数的浅层。
     Frozen:在训练我们的任务B的时候浅层参数不变
     Fine-tuning： 就是更好地把参数进行调整使得更适应当前的 C 任务。一般图像或者视频领域要做预训练一般都这么做
   为什么可以用？
    因为观察所有模型底层，发现底层学习到的特征打印发现都是最基础的线条信息，是可以广泛使用的
    一般用imageNet做预训练，数据多，种类多
  ```
  
  b)NLP预训练
  ----------------
  ```bash
    Word Embedding
    就是当年做语言模型(根据前文判断一句话是人话的概率)的副产品,相当于模型第一层的参数
    Word2Vec有两种方法，CBOW或者Skipgram ，word2vec是根据上下文了，为什么这么随性？因为这个模型的目的不是为了做什么任务，
    而是为了得到word embedding这一层的参数。
    所以和图像类似，不过word embedding是最底层的一层参数
    如何使用？
      句子每个单词onehot输入，乘以学好的word embedding矩阵Q
    这是2018年之前的方法，WE有个问题是无法区分多义词   play有玩和播放的意思，但是WE里都是一个向量表示,所以提升效果不大
    
  ```
  
  2.从WE进化到ELMO
  =====================
  
  a)ELMO思想：
  ----------------
  ```bash
    我事先用语言模型学好一个单词的 Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用 Word Embedding 的时候，
    单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的 Word Embedding 表示，
    这样经过调整后的 Word Embedding 更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了
    
  ```
  b)ELMO结构
  -----------------------
  ```bash
  两部分：语言模型帷幕的的预训练和下游任务部分
  1.预训练是双层双向LSTM：
  根据上下文预测wi
  先用预训练部分训练大量语料，然后输入一个新句子，每个单词会得到3个embedding：1.底层word embedding 2.LSTM中对应单词位置的 Embedding，句法信息更多一些
  3.第二层 LSTM 中对应单词位置的 Embedding，这层编码单词的语义信息更多一些
  然后每个单词3个embedding加权求和作为下游任务输入，这个预训练也叫Feature-based Pre-Training
  2.下游任务
  
  解决的多意问题
  弱点：BILSTM抽取特征能力没有transform厉害
 
  ```
  
  3.GPT generative pre training
  ======================
  
  1.结构
  ----------
  ```bash
  也是两层，语言模型为目的的预训练和通过fine-tuning模式解决下游任务，类似于ELMO不同在于
  a)特征提取使用了transformer，强于LSTM transform易于并行，长距离特征强
  b)预训练没使用上下文而是只使用了上文(目前看来不太好，还是上下文比较好)
  
  第二层必须和GPT网络结构一样，论文有施工图
  ```
  
  4.BERT
  ==========================
  ```bash
  也是两层
  1.和GPT的区别就是预训练采用双向的语言模型，类似于ELMO
 2.第二层就是Fine-turning模式解决下游任务，他的预训练语言模型使用的数据大
 
 所以下游任务也要改造结构
 ```
 
 NLP四大类任务
 --------------
 ```bash
 a)序列标注： 分词/POS tag /NER/语义标注
 b)分类任务： 文本分类/情感分析
 c)句子关系判断： entailment/QA/自然语言推理
 d)生成式任务：机器翻译/文本摘要
 
 ```
 BERT如何构造双向语言模型
 ----------------
 ```bash
 1.Masked双向语言模型
 Masked 双向语言模型向上图展示这么做：随机选择语料中 15% 的单词，把它抠掉，也就是用 [Mask] 掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。
 但是这里有个问题：训练过程大量看到 [mask] 标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对 [mask] 这个标记的，
 但是实际使用又见不到这个标记，这自然会有问题。

为了避免这个问题，Bert 改造了一下，15% 的被上天选中要执行 [mask] 替身这项光荣任务的单词中，只有 80% 真正被替换成 [mask] 标记，
10% 被狸猫换太子随机替换成另外一个单词，10% 情况这个单词还待在原地不做改动。这就是 Masked 双向语音模型的具体做法。

2.Next Sentence Prediction
指的是做语言模型预训练的时候，分两种情况选择两个句子，一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，
随机选择一个拼到第一个句子后面。我们要求模型除了做上述的 Masked 语言模型任务外，附带再做个句子关系预测，
判断第二个句子是不是真的是第一个句子的后续句子。

之所以这么做，是考虑到很多 NLP 任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。
所以可以看到，它的预训练是个多任务过程。这也是 Bert 的一个创新。

其实NSP效果不大，只是对特定任务有影响

 ```
 改造下游任务
 ------------------
 ```bash
 1.对于句子关系类任务，很简单，和 GPT 类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，
 把第一个起始符号对应的 Transformer 最后一层位置上面串接一个 softmax 分类层即可。
 2.对于分类问题，与 GPT 一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造
 3.对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分 Transformer 最后一层每个单词对应位置都进行分类即可
 4.生成没提，。只需要附着在 S2S 结构上，encoder 部分是个深度 Transformer 结构，decoder 部分也是个深度 Transformer 结构。
 根据任务选择不同的预训练数据初始化 encoder 和 decoder 即可。
 4.
 ```
 
 注意力
 =====================
 
 0.encoder-decoder历史发展
 --------------------------
 ```bash
 a)处理序列问题(N v N) 输入输出等长
  输入x序列得到y序列
  
  h0 -> h1 -> h2 -> h3 -> h4
        ^     ^     ^     ^
        |     |     |     |
        x1    x2    x3    x4
        
        h1 = f(Ux1 + Wh0 + b)   h2 = f(Ux2 + Wh1 + b)
        
   然后：
         y1    y2
         ^     ^
         |     |
   h0 -> h1 -> h2 -> h3 -> h4
        ^     ^     ^     ^
        |     |     |     |
        x1    x2    x3    x4
 y1 = softmax(Vh1 + c)
 
 
 b) N v 1分类问题
 
                           y
                           ^
                           |
   h0 -> h1 -> h2 -> h3 -> h4
        ^     ^     ^     ^
        |     |     |     |
        x1    x2    x3    x4
 
 y = softmax(Vh4 + c)
 
 c) 1 V N 图像生成文字
 
        y1    y2
         ^     ^
         |     |
   h0 -> h1 -> h2 -> h3 -> h4
        ^     ^     ^     ^
        |     |     |     |
        x     x     x     x 
        
        
  d) N v M  也叫encoder decoder，输入输出不等长
  
                            
                                       y1    y2     y3
  
   h0 -> h1 -> h2 -> h3 -> h4 -> c -> h11 -> h12 -> h13
        ^     ^     ^     ^
        |     |     |     |
        x1    x2    x3    x4
        
   c有很多种算法:
    c = h4
    c = q(h4)
  e)因为c存不下太多信息，所以当序列太长，精度会下降，所以发明attention思想
  
                           
                                              y1    y2     y3
  
   h0 -> h1 -> h2 -> h3 -> h4 c1 c2 c3   h0'-> h11 -> h12 -> h13
        ^     ^     ^     ^                    ^     ^     ^ 
        |     |     |     |                    |     |     |
        x1    x2    x3    x4                   c1    c2    c3
        
        c1 = h1* a11 + h2*a12 + h3*a13 + h4*a14    a就是每个字权重
        
        a11 = f(h1,h0')  a12 = f(h2,h0')
        a21 = f(h1,h11)
```
 
 1.encoder -decoder框架
 ----------------
 ```bash
 注意力不一定需要依赖encoder decoder框架，他是一种思想
 1.如果没有注意力-
 source(x1,x2,x3) -> target(y1,y2,y3)
 翻译 Tom Chase Jerry -> 汤姆 追逐 杰瑞
 source的任意词对于生成target的词 yi 影响力是一样的，
 y1 = f(C), y2 = f(C,y1) , y3 = f(C,y1y2) 他们的语义编码C都是一样的
 f是Decoder的非线性变换函数
 
 2.有注意力
 但是其实杰瑞受jerry影响最大
 y1 = f(C1)
 y2 = f(C2,y1)
 y3 = f(C3,y1,y2)
 C汤姆 = g(0.6 * f2(Tom),0.2 * f2(Chase), 0.2* f2(Jerry))
 C追逐 = g(0.2 * f2(Tom),0.7 * f2(Chase), 0.1* f2(Jerry))
 C杰瑞 = g(0.3 * f2(Tom),0.2 * f2(Chase), 0.5* f2(Jerry))
 f2函数代表Encoder对输入英文单词的某种变换函数
 比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入xi后隐层节点的状态值
 g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和
 
 ```
 2.Attention实质（思想）
 ------------------
 ```bash
 将Source中的构成元素想象成是由一系列的<Key,Value>数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性
 ，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。
 所以本质上Attention机制是对Source中元素的Value值进行加权求和


总结分3步:1.计算yi和每个x相似性
         2.类softmax行为做个归一化得到权重 
         3.每个x对应value的加权求和得到attention，进入f()得到yi
         
         其中1有很多方法，点积啊，cosine similarity啊之类的或者更牛逼的用额外神经网络
```
![](https://github.com/ehamster/NLP/blob/master/images/attentions.png)

3.self attention
-----------------

上面说的attention是发生在target和source种，self-attention发生在source内部或者target内部的
也可以说是source = target的情况下
比如
The law is important because it is useful
it的attention在law上有很高的权重

因为他是直接计算两个词之间相似度的，所以不像LSTM受到距离影响。
