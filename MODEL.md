模型学习方法，会其意，知其形。先动他的目的的理念，再看公式
================
![](https://github.com/ehamster/NLP/blob/master/images/conclusion.jpg)

监督学习的模型分为 判别式模型(discriminative model) 和生成式模型(generative model)
------------
```bash
1. A批模型是这么工作的，他们直接将数据的Y（或者label），根据所提供的features，学习，最后画出了一个明显或者比较明显的边界（具体怎么做到的？通过复杂的函数映射，或者决策叠加等等mechanism），这一点线性LR、线性SVM应该很明显吧。 2. B批模型是这么工作的，他们先从训练样本数据中，将所有的数据的分布情况摸透，然后最终确定一个分布，来作为我的所有的输入数据的分布，并且他是一个联合分布 P(X,Y)(注意 X包含所有的特征 xi， Y包含所有的label)。然后我来了新的样本数据（inference），好，通过学习来的模型的联合分布 P(X,Y)，再结合新样本给的X ，通过条件概率就能出来 Y ：P(Y|X)=P(X,Y) / P(X)

作者：Scofield
链接：https://www.zhihu.com/question/35866596/answer/236886066
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

判别式模型：
1.对 P(Y|X) 建模
2.对所有的样本只构建一个模型，确认总体判别边界
3.观测到输入什么特征，就预测最可能的label
4.另外，判别式的优点是：对数据量要求没生成式的严格，速度也会快，小数据量下准确率也会好些。

同样，B批模型对应了生成式模型。并且需要注意的是，在模型训练中，我学习到的是X与Y的联合模型p(Y|X)  ，也就是说，我在训练阶段是只对P(Y|X)建模，我需要确定维护这个联合概率分布的所有的信息参数。完了之后在inference再对新的sample计算  ，导出  ,但这已经不属于建模阶段了。

1.对P(X,Y)建模
2.这里我们主要讲分类问题，所以是要对每个label（ yi）都需要建模，最终选择最优概率的label为结果，所以没有什么判别边界。（对于序列标注问题，那只需要构件一个model）
3. 中间生成联合分布，并可生成采样数据。
4.生成式模型的优点在于，所包含的信息非常齐全，我称之为“上帝信息”，所以不仅可以用来输入label，还可以干其他的事情。生成式模型关注结果是如何产生的。但是生成式模型需要非常充足的数据量以保证采样到了数据本来的面目，所以速度相比之下，慢。


```
1.最大熵模型：
---------------
  分担风险，考虑所有可能，但是实现复杂效率低
  
  
2.HMM隐马尔科夫： 生成式模型
--------------

```bash
  我们使用一个隐马尔科夫模型（HMM）对这些例子建模。这个模型包含两组状态集合和三组概率集合：
　　* 隐藏状态：一个系统的（真实）状态，可以由一个马尔科夫过程进行描述（例如，天气）。
　　* 观察状态：在这个过程中‘可视’的状态（例如，海藻的湿度）。
　　* pi向量：包含了（隐）模型在时间t=1时一个特殊的隐藏状态的概率（初始概率）。
　　* 状态转移矩阵：包含了一个隐藏状态到另一个隐藏状态的概率
　　* 混淆矩阵：包含了给定隐马尔科夫模型的某一个特殊的隐藏状态，观察到的某个观察状态的概率。
　　因此一个隐马尔科夫模型是在一个标准的马尔科夫过程中引入一组观察状态，以及其与隐藏状态间的一些概率关系。
  
   一旦一个系统可以作为HMM被描述，就可以用来解决三个基本问题。其中前两个是模式识别的问题：给定HMM求一个观察序列的概率（评估）；
   搜索最有可能生成一个观察序列的隐藏状态序列（解码）。第三个问题是给定观察序列生成一个HMM（学习）。
 ```
   
   🌰：
   --------
   ```bash
   x:         john saw the saw
              |     |   |   |
   y: start-> PN    V   D   N ->end
   
   p(x,y) = p(y)p(x|y)
   
   P(y) = P(PN|start)*P(V|PN) * P(D|V) * P(end|N)
   P(x|y) = P(john|PN) * P(saw|V) * P(the|D) * P(saw|N)
   HMM只考虑1gram，这就是他的弊端
   ```
   ![](https://github.com/ehamster/NLP/blob/master/images/Screenshot%202019-03-15%20at%2014.57.36.png)
   
   3.CRF 条件随即场  conditional random field
   ------------------
   ```bash
   最简单版本：线性的。
   特征函数接受4个参数:  返回0或者1
   1.句子s
   2.第i个词
   3.第i个词的词性
   4.第i-1个词的词性
   
   许多特征函数得出评分，就能得出句子的probability
   λ是一个系数
   ```
![](https://github.com/ehamster/NLP/blob/master/images/Screenshot%202019-03-18%20at%2009.23.44.png)

4.Word2Vec
-----------------
```bash
他是word embedding的一种

如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』
而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』
eg  CBOW根据上下文求中心词
上下文的onehotencoding * w 就得到word2vec。  worc2vec * w‘ 得到中心词
比如，在skipgram来说：
  input: 1xV one hot encoding   假设一共V个词
  hidern layer : VxN W  N远远小于V
  output: 1XN * NxV W'  = 1XV
```
